{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of YOLOv5 Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cef5e9351ca743bcba5febac0b096a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec326c52378f4410920c328f221e0514",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_83000c64a11c4ae8abd6f0ef2f108cef",
              "IPY_MODEL_0f7899eb719f4a9c9852426551f97be9"
            ]
          }
        },
        "ec326c52378f4410920c328f221e0514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83000c64a11c4ae8abd6f0ef2f108cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_886ac5b18b3c4c82bf15ad5055f1e17e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 819257867,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 819257867,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e67b3c3a49849c7a7ba28b7eec96e7a"
          }
        },
        "0f7899eb719f4a9c9852426551f97be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62c3682ff1804571a483d46664533969",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 781M/781M [00:12&lt;00:00, 67.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_599dda3b608b432393760b2ca4ae7c7d"
          }
        },
        "886ac5b18b3c4c82bf15ad5055f1e17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e67b3c3a49849c7a7ba28b7eec96e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62c3682ff1804571a483d46664533969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "599dda3b608b432393760b2ca4ae7c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "217ca488c82a4b7a80318b70887a556e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e63af16f1084ca98a6fa5a282f2a81e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49f4b3c7f6ff42b4b9132a8550e12186",
              "IPY_MODEL_8ec9e1a4883245daaf029458ee09721f"
            ]
          }
        },
        "4e63af16f1084ca98a6fa5a282f2a81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49f4b3c7f6ff42b4b9132a8550e12186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d3e775ee11e4cf4b587b64fbc3cc6f7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 22091032,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 22091032,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70f68a9a51ac46e6ab7e51fb4fc6bda3"
          }
        },
        "8ec9e1a4883245daaf029458ee09721f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fdb8ab377c114bc3b862ba76eb93cef7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21.1M/21.1M [00:36&lt;00:00, 605kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd267c153c244621a1f50706d2ddc897"
          }
        },
        "9d3e775ee11e4cf4b587b64fbc3cc6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70f68a9a51ac46e6ab7e51fb4fc6bda3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdb8ab377c114bc3b862ba76eb93cef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd267c153c244621a1f50706d2ddc897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandee5393/100pip/blob/main/Copy_of_YOLOv5_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<a align=\"left\" href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
        "<img src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/splash_horizontal.png\"></a>\n",
        "\n",
        "This is the **official YOLOv5 ðŸš€ notebook** authored by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
        "For more information please visit https://github.com/ultralytics/yolov5 and https://ultralytics.com. Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ad75ce-86b0-4b5c-d1c1-8aa2376f1b65"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 (Tesla K80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf6DPt0R6Y09"
      },
      "source": [
        "!unzip -q ../train_data.zip -d ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Inference\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/114307955-5c7e4e80-9ae2-11eb-9f50-a90e39bee53f.png\" width=\"900\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaa1268-3a40-4c8f-f4f6-044bdc8cda55"
      },
      "source": [
        "!python detect.py --weights runs/train/exp/weights/last.pt --img 640 --conf 0.25 --source /content/yolov5/runs/detect/exp/D.jpg\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/last.pt'], source=/content/yolov5/runs/detect/exp/D.jpg, imgsz=640, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 ðŸš€ v5.0-273-g850970e torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "Fusing layers... \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n",
            "image 1/1 /content/yolov5/runs/detect/exp/D.jpg: 640x480 1 100 PIPER, Done. (0.029s)\n",
            "Results saved to runs/detect/exp2\n",
            "Done. (0.099s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxtpkFDdDHgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f76335-49c8-434b-8eae-25072b8ae8f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Test\n",
        "Test a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTZYGgRjnMc"
      },
      "source": [
        "## COCO val2017\n",
        "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "cef5e9351ca743bcba5febac0b096a30",
            "ec326c52378f4410920c328f221e0514",
            "83000c64a11c4ae8abd6f0ef2f108cef",
            "0f7899eb719f4a9c9852426551f97be9",
            "886ac5b18b3c4c82bf15ad5055f1e17e",
            "4e67b3c3a49849c7a7ba28b7eec96e7a",
            "62c3682ff1804571a483d46664533969",
            "599dda3b608b432393760b2ca4ae7c7d"
          ]
        },
        "outputId": "56b6402a-81d5-41d0-a3c8-8889db1fca6c"
      },
      "source": [
        "# Download COCO val2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cef5e9351ca743bcba5febac0b096a30",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=819257867.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d41761-f1a0-41fe-d0bb-4cceebd7c4a6"
      },
      "source": [
        "# Run YOLOv5x on COCO val2017\n",
        "!python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', exist_ok=False, half=True, img_size=640, iou_thres=0.65, name='exp', project='runs/test', save_conf=False, save_hybrid=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt'])\n",
            "YOLOv5 ðŸš€ v5.0-157-gc6b51f4 torch 1.8.1+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n",
            "100% 168M/168M [00:01<00:00, 156MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 3008.87it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../coco/val2017.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:17<00:00,  2.02it/s]\n",
            "                 all       5000      36335      0.746      0.626       0.68       0.49\n",
            "Speed: 5.3/1.5/6.8 ms inference/NMS/total per 640x640 image at batch-size 32\n",
            "\n",
            "Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=4.88s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=83.47s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=12.96s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n",
            "Results saved to runs/test/exp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_KbFk0juX2"
      },
      "source": [
        "## COCO test-dev2017\n",
        "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0AJnSeCIHyJ"
      },
      "source": [
        "# Download COCO test-dev2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip # unzip labels\n",
        "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n",
        "%mv ./test2017 ../coco/images  # move to /coco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29GJXAP_lPrt"
      },
      "source": [
        "# Run YOLOv5s on COCO test-dev2017 using --task test\n",
        "!python test.py --weights yolov5s.pt --data coco.yaml --task test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knxi2ncxWffW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "217ca488c82a4b7a80318b70887a556e",
            "4e63af16f1084ca98a6fa5a282f2a81e",
            "49f4b3c7f6ff42b4b9132a8550e12186",
            "8ec9e1a4883245daaf029458ee09721f",
            "9d3e775ee11e4cf4b587b64fbc3cc6f7",
            "70f68a9a51ac46e6ab7e51fb4fc6bda3",
            "fdb8ab377c114bc3b862ba76eb93cef7",
            "cd267c153c244621a1f50706d2ddc897"
          ]
        },
        "outputId": "9e4788c2-e1d4-4a13-c3d2-984f5df7ffab"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "217ca488c82a4b7a80318b70887a556e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=22091032.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOkGLv1dMqh"
      },
      "source": [
        "Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
        "\n",
        "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "source": [
        "# Tensorboard  (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLAV42oNb7M"
      },
      "source": [
        "# Weights & Biases  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04be79c8-e850-4b51-d6cc-e70ebf49f80e"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 60 --data custom_data.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=60, batch_size=16, img_size=[640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=None, bucket=, cache_images=True, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v5.0-273-g850970e torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2021-07-07 17:16:16.181598: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 54.0MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs\n",
            "\n",
            "Transferred 356/362 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.0 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../train_data/labels/train' images and labels...28 found, 0 missing, 0 empty, 6 corrupted: 100% 28/28 [00:00<00:00, 663.53it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0019.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0020 - Copy.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0020.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0021 - Copy.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0021.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/IMG-20210621-WA0022 - Copy.jpg: negative labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../train_data/labels/train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB): 100% 22/22 [00:00<00:00, 86.12it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../train_data/labels/val' images and labels...7 found, 0 missing, 0 empty, 0 corrupted: 100% 7/7 [00:00<00:00, 248.15it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../train_data/labels/val.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB): 100% 7/7 [00:00<00:00, 42.22it/s]\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.22, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      0/59     3.21G    0.1295   0.05322         0    0.1827        56       640: 100% 2/2 [00:04<00:00,  2.21s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.90it/s]\n",
            "                 all          7         40     0.0223      0.625      0.015    0.00323\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      1/59     3.21G    0.1009   0.05077         0    0.1516        41       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.32it/s]\n",
            "                 all          7         40     0.0275      0.175     0.0152    0.00362\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      2/59     3.21G    0.1283   0.05588         0    0.1841        55       640: 100% 2/2 [00:01<00:00,  1.33it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.36it/s]\n",
            "                 all          7         40     0.0323      0.225     0.0175    0.00435\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      3/59     3.21G    0.1208   0.05304         0    0.1738        50       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.32it/s]\n",
            "                 all          7         40     0.0383      0.225     0.0205    0.00594\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      4/59     3.21G    0.1198   0.05503         0    0.1748        60       640: 100% 2/2 [00:01<00:00,  1.33it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.42it/s]\n",
            "                 all          7         40     0.0359      0.225     0.0218    0.00613\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      5/59     3.21G    0.1233   0.05057         0    0.1739        58       640: 100% 2/2 [00:01<00:00,  1.33it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.34it/s]\n",
            "                 all          7         40     0.0439      0.325     0.0261    0.00795\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      6/59     3.21G    0.1219   0.05489         0    0.1767        43       640: 100% 2/2 [00:01<00:00,  1.33it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.28it/s]\n",
            "                 all          7         40     0.0503      0.325     0.0308    0.00894\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      7/59     3.21G    0.1249   0.05161         0    0.1765        44       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.49it/s]\n",
            "                 all          7         40     0.0561      0.375     0.0374    0.00927\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      8/59     3.21G    0.1243   0.04999         0    0.1743        45       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.45it/s]\n",
            "                 all          7         40     0.0734      0.325     0.0519    0.00964\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      9/59     3.21G    0.1126   0.05324         0    0.1659        37       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.48it/s]\n",
            "                 all          7         40      0.111      0.194     0.0615     0.0123\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     10/59     3.21G    0.1122   0.06151         0    0.1738        46       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.36it/s]\n",
            "                 all          7         40      0.134        0.2     0.0761     0.0171\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     11/59     3.21G    0.1171    0.0605         0    0.1776        54       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.58it/s]\n",
            "                 all          7         40      0.168      0.225      0.092     0.0219\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     12/59     3.21G   0.09159   0.04507         0    0.1367        35       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.55it/s]\n",
            "                 all          7         40      0.206      0.225       0.12     0.0293\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     13/59     3.21G    0.1065   0.06474         0    0.1712        60       640: 100% 2/2 [00:01<00:00,  1.33it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.41it/s]\n",
            "                 all          7         40      0.279      0.275      0.164     0.0425\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     14/59     3.21G    0.1014   0.06512         0    0.1666        44       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.49it/s]\n",
            "                 all          7         40      0.328      0.275        0.2     0.0566\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     15/59     3.21G    0.1065   0.08085         0    0.1873        68       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.49it/s]\n",
            "                 all          7         40      0.488      0.275      0.249     0.0741\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     16/59     3.21G   0.09468     0.062         0    0.1567        45       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.54it/s]\n",
            "                 all          7         40        0.6      0.275      0.286     0.0797\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     17/59     3.21G   0.09833   0.07108         0    0.1694        54       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.64it/s]\n",
            "                 all          7         40      0.608      0.272      0.311     0.0872\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     18/59     3.21G   0.09208   0.07331         0    0.1654        50       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.60it/s]\n",
            "                 all          7         40      0.548      0.275      0.311     0.0893\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     19/59     3.21G   0.08499   0.06967         0    0.1547        54       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.37it/s]\n",
            "                 all          7         40      0.281      0.725      0.368     0.0988\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     20/59     3.21G   0.08521   0.07639         0    0.1616        61       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.59it/s]\n",
            "                 all          7         40      0.334        0.7      0.404      0.121\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     21/59     3.21G   0.07775   0.08304         0    0.1608        65       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.64it/s]\n",
            "                 all          7         40      0.435       0.55      0.451       0.15\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     22/59     3.21G   0.08174   0.08562         0    0.1674        66       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.63it/s]\n",
            "                 all          7         40      0.438      0.675      0.525      0.157\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     23/59     3.21G   0.07391   0.07141         0    0.1453        50       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.68it/s]\n",
            "                 all          7         40      0.394      0.775      0.569        0.2\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     24/59     3.21G   0.07483   0.06736         0    0.1422        46       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.66it/s]\n",
            "                 all          7         40      0.469       0.64      0.611      0.219\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     25/59     3.21G   0.07583   0.07205         0    0.1479        44       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.71it/s]\n",
            "                 all          7         40      0.517       0.75      0.679       0.25\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     26/59     3.21G   0.07413   0.08758         0    0.1617        63       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.62it/s]\n",
            "                 all          7         40      0.648       0.65      0.713      0.273\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     27/59     3.21G   0.07075    0.0623         0    0.1331        35       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.75it/s]\n",
            "                 all          7         40      0.626       0.65      0.729      0.281\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     28/59     3.21G     0.069   0.07495         0    0.1439        57       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.79it/s]\n",
            "                 all          7         40      0.568       0.85      0.748      0.277\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     29/59     3.21G   0.07137    0.0713         0    0.1427        52       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.66it/s]\n",
            "                 all          7         40       0.63      0.875      0.775      0.291\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     30/59     3.21G   0.06549   0.08039         0    0.1459        57       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "                 all          7         40      0.646      0.823      0.784      0.307\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     31/59     3.21G   0.06914   0.09485         0     0.164        74       640: 100% 2/2 [00:01<00:00,  1.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.79it/s]\n",
            "                 all          7         40       0.66      0.824      0.799      0.311\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     32/59     3.21G   0.07104   0.08852         0    0.1596        63       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.63it/s]\n",
            "                 all          7         40      0.679       0.85      0.802      0.315\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     33/59     3.21G   0.06539   0.08119         0    0.1466        63       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.59it/s]\n",
            "                 all          7         40       0.68      0.825      0.805      0.337\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     34/59     3.21G   0.06605   0.07997         0     0.146        58       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.73it/s]\n",
            "                 all          7         40      0.686      0.821      0.804      0.336\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     35/59     3.21G    0.0677   0.06558         0    0.1333        49       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.71it/s]\n",
            "                 all          7         40      0.716      0.825      0.815      0.361\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     36/59     3.21G   0.06486   0.07124         0    0.1361        55       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.77it/s]\n",
            "                 all          7         40      0.711      0.922       0.82      0.379\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     37/59     3.21G   0.06844   0.08993         0    0.1584        74       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.77it/s]\n",
            "                 all          7         40       0.71      0.925      0.817      0.384\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     38/59     3.21G   0.06473   0.08347         0    0.1482        69       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.85it/s]\n",
            "                 all          7         40      0.753      0.925      0.838      0.406\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     39/59     3.21G   0.06443   0.07204         0    0.1365        55       640: 100% 2/2 [00:01<00:00,  1.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.78it/s]\n",
            "                 all          7         40      0.787      0.924      0.871      0.428\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     40/59     3.21G   0.06256   0.06597         0    0.1285        51       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.83it/s]\n",
            "                 all          7         40      0.818        0.9      0.883       0.44\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     41/59     3.21G   0.05492   0.06847         0    0.1234        48       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.84it/s]\n",
            "                 all          7         40      0.818        0.9      0.907      0.455\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     42/59     3.21G   0.06316   0.07786         0     0.141        59       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.66it/s]\n",
            "                 all          7         40      0.836        0.9      0.918      0.473\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     43/59     3.21G   0.06085   0.06772         0    0.1286        51       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "                 all          7         40      0.814        0.9      0.916      0.473\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     44/59     3.21G   0.06212   0.06554         0    0.1277        61       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.60it/s]\n",
            "                 all          7         40      0.836        0.9       0.92      0.478\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     45/59     3.21G   0.06106   0.07505         0    0.1361        64       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.69it/s]\n",
            "                 all          7         40        0.9      0.898      0.941      0.493\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     46/59     3.21G    0.0621   0.06535         0    0.1275        38       640: 100% 2/2 [00:01<00:00,  1.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.79it/s]\n",
            "                 all          7         40      0.871       0.85      0.912      0.479\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     47/59     3.21G   0.06199   0.05588         0    0.1179        48       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.75it/s]\n",
            "                 all          7         40      0.944      0.849      0.947        0.5\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     48/59     3.21G    0.0644   0.07265         0     0.137        65       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.71it/s]\n",
            "                 all          7         40      0.919       0.85      0.941      0.499\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     49/59     3.21G   0.05373   0.06269         0    0.1164        37       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.71it/s]\n",
            "                 all          7         40       0.86      0.924      0.942      0.497\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     50/59     3.21G   0.05916   0.06728         0    0.1264        54       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "                 all          7         40       0.86      0.925      0.946      0.505\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     51/59     3.21G   0.06203   0.06514         0    0.1272        51       640: 100% 2/2 [00:01<00:00,  1.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.90it/s]\n",
            "                 all          7         40       0.86      0.924      0.936      0.503\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     52/59     3.21G   0.06334   0.05185         0    0.1152        30       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.86it/s]\n",
            "                 all          7         40      0.947      0.899      0.954      0.522\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     53/59     3.21G   0.05856   0.06245         0     0.121        52       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.84it/s]\n",
            "                 all          7         40      0.946        0.9      0.955      0.526\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     54/59     3.21G   0.05487   0.08641         0    0.1413        69       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.88it/s]\n",
            "                 all          7         40      0.898        0.9      0.941      0.512\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     55/59     3.21G   0.05898   0.07539         0    0.1344        60       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.87it/s]\n",
            "                 all          7         40        0.9      0.898      0.942      0.517\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     56/59     3.21G   0.05752   0.06333         0    0.1209        43       640: 100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.86it/s]\n",
            "                 all          7         40        0.9      0.899      0.945      0.527\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     57/59     3.21G    0.0445    0.0561         0    0.1006        42       640: 100% 2/2 [00:01<00:00,  1.32it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.74it/s]\n",
            "                 all          7         40       0.92      0.875      0.952      0.538\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     58/59     3.21G   0.05777   0.07287         0    0.1306        68       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.89it/s]\n",
            "                 all          7         40      0.921      0.874      0.952       0.53\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     59/59     3.21G   0.05643   0.06802         0    0.1245        58       640: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.73it/s]\n",
            "                 all          7         40      0.878        0.9       0.95      0.532\n",
            "60 epochs completed in 0.045 hours.\n",
            "\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging ðŸŒŸ NEW\n",
        "\n",
        "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPdhraOTCO0"
      },
      "source": [
        "Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n",
        "Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # test batch 0 labels\n",
        "Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # test batch 0 predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYG4WFEnTVrI"
      },
      "source": [
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"750\">  \n",
        "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"750\">  \n",
        "`test_batch0_labels.jpg` shows test batch 0 labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"750\">  \n",
        "`test_batch0_pred.jpg` shows test batch 0 _predictions_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training results are automatically logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and `runs/train/exp/results.txt`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.txt` file manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDznIqPF7nk3"
      },
      "source": [
        "from utils.plots import plot_results \n",
        "plot_results(save_dir='runs/train/exp')  # plot all results*.txt files in 'runs/train/exp'\n",
        "Image(filename='runs/train/exp/results.png', width=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrEegCSW3fK"
      },
      "source": [
        "<p align=\"left\"><img width=\"800\" alt=\"COCO128 Training Results\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/results_COCO128.png\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKoSIK2WSzj"
      },
      "source": [
        "# Reproduce\n",
        "for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n",
        "  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n",
        "  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "source": [
        "# PyTorch Hub\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Images\n",
        "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
        "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(imgs)\n",
        "results.print()  # or .show(), .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGH0ZjkGjejy"
      },
      "source": [
        "# Unit tests\n",
        "%%shell\n",
        "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
        "\n",
        "rm -rf runs  # remove runs/\n",
        "for m in yolov5s; do  # models\n",
        "  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n",
        "  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n",
        "  for d in 0 cpu; do  # devices\n",
        "    python detect.py --weights $m.pt --device $d  # detect official\n",
        "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
        "    python test.py --weights $m.pt --device $d # test official\n",
        "    python test.py --weights runs/train/exp/weights/best.pt --device $d # test custom\n",
        "  done\n",
        "  python hubconf.py  # hub\n",
        "  python models/yolo.py --cfg $m.yaml  # inspect\n",
        "  python export.py --weights $m.pt --img 640 --batch 1  # export\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogI-kwi3Tye"
      },
      "source": [
        "# Profile\n",
        "from utils.torch_utils import profile \n",
        "\n",
        "m1 = lambda x: x * torch.sigmoid(x)\n",
        "m2 = torch.nn.SiLU()\n",
        "profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVRSOhEvUdb5"
      },
      "source": [
        "# Evolve\n",
        "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
        "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSgFCAcMbk1R"
      },
      "source": [
        "# VOC\n",
        "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
        "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}